# configs/default.yaml - Configuration principale pour Avikam1 LLMversion: 1.2.0model:  name: "evilafo/avikam1-7b"  # Nom/path du modèle HF  torch_dtype: "bfloat16"      # Précision: float32|bfloat16|float16  device_map: "auto"           # Stratégie de placement sur GPU/CPU  attn_implementation: null    # "flash_attention_2" si disponible  # Paramètres LoRA  use_lora: true               # Activer LoRA  lora_rank: 64                # Rang de décomposition (8-128)  lora_alpha: 32               # Paramètre alpha (ratio scaling)  lora_target_modules:         # Modules cibles    - "q_proj"    - "v_proj"    - "k_proj"    - "o_proj"  lora_dropout: 0.05           # Dropout des couches LoRAtraining:  # Hyperparamètres  per_device_train_batch_size: 8    # Batch size par GPU  gradient_accumulation_steps: 2    # Accumulation pour grands batchs  num_train_epochs: 3               # Nombre d'époques  learning_rate: 2e-5               # Taux d'apprentissage  weight_decay: 0.01                # Régularisation L2  warmup_ratio: 0.05                # Warmup proportionnel  # Stratégies  optim: "adamw_torch_fused"        # Optimiseur  lr_scheduler_type: "cosine"       # Cosine avec restart  gradient_checkpointing: true      # Économie de mémoire  fp16: false                       # À désactiver en bfloat16  bf16: true                        # Meilleur que fp16 sur récentes GPUs  # Sauvegarde  save_strategy: "steps"            # Sauvegarde par pas  save_steps: 500                   # Fréquence de sauvegarde  evaluation_strategy: "steps"       # Évaluation pendant l'entraînement  eval_steps: 500                   # Fréquence d'évaluationdata:  # Chemins des datasets  train_path: "data/processed/train.json"    # Données d'entraînement  val_path: "data/processed/val.json"        # Données de validation  test_path: "data/processed/test.json"      # Données de test (optionnel)    # Paramètres de tokenization  context_length: 2048              # Longueur maximale de contexte  padding: "max_length"             # "max_length"|"longest"  truncation: true                  # Troncation des séquences longueslogging:  level: "INFO"                     # DEBUG|INFO|WARNING|ERROR  format: "%(asctime)s - %(levelname)s - %(message)s"  wandb: false                      # Activer Weights & Biases  wandb_project: "avikam1"          # Nom du projet W&B# Configuration DeepSpeed (optionnel)deepspeed:  enabled: false  config_path: "configs/ds_config.json"  # Config pour multi-GPU